{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e925216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 18:09:34.492934: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-09 18:09:35.446306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7859 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/barath/miniconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd168181f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../dependencies/')\n",
    "import dataset_utils\n",
    "import network_lrat_code as network\n",
    "import utils\n",
    "import zca\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "train_set = 'celeb_a'\n",
    "norm = None\n",
    "mode = 'color'\n",
    "which_arch = ''\n",
    "bg = 1\n",
    "\n",
    "if mode == 'color':\n",
    "    input_shape = (32, 32, 3)\n",
    "    datasets = [\n",
    "        'svhn_cropped',\n",
    "        'cifar10',\n",
    "        'celeb_a',\n",
    "        'gtsrb',\n",
    "        'compcars',\n",
    "        'noise'\n",
    "    ]\n",
    "    num_filters = 64\n",
    "    if bg:\n",
    "        mutation_rate = 0.1\n",
    "    else:\n",
    "        mutation_rate = 0\n",
    "elif mode == 'grayscale':\n",
    "    input_shape = (32, 32, 1)\n",
    "    datasets = [\n",
    "    'mnist',\n",
    "    'fashion_mnist',\n",
    "    'emnist/letters',\n",
    "    'sign_lang',\n",
    "    'noise'\n",
    "    ]\n",
    "    num_filters = 32\n",
    "    if bg:\n",
    "        mutation_rate = 0.3\n",
    "    else:\n",
    "        mutation_rate = 0\n",
    "    \n",
    "if which_arch == 'lrat_arch':\n",
    "    if bg:\n",
    "        pre = 'lrat_arch_bg_reg/'\n",
    "        reg_weight = 100\n",
    "    else:\n",
    "        pre = 'lrat_arch/'\n",
    "        reg_weight = 0\n",
    "    num_resnet = 5\n",
    "    num_hierarchies = 1\n",
    "    num_logistic_mix = 10\n",
    "    num_filters = 160\n",
    "    dropout_p = 0\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.0001,\n",
    "        decay_steps=1,\n",
    "        decay_rate=0.999995)\n",
    "    epochs = 427\n",
    "    use_weight_norm = False\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.95, beta_2=0.9995)\n",
    "else:\n",
    "    if bg:\n",
    "        pre = 'lrat_bg_reg/'\n",
    "        reg_weight = 0\n",
    "    else:\n",
    "        pre = 'lrat/'\n",
    "        reg_weight = 0\n",
    "    num_resnet = 2\n",
    "    num_hierarchies = 4\n",
    "    num_logistic_mix = 5\n",
    "    num_filters = num_filters\n",
    "    dropout_p = 0.3\n",
    "    learning_rate = 1e-3\n",
    "    use_weight_norm = True\n",
    "    epochs = 100\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "if norm is None:\n",
    "    dir_str = 'original'\n",
    "elif norm == 'pctile-5':\n",
    "    dir_str = 'pctile-5'\n",
    "elif norm == 'channelwhiten':\n",
    "    dir_str = 'zca'\n",
    "elif norm == 'zca_original':\n",
    "    dir_str = 'zca_original'\n",
    "elif norm == 'histeq':\n",
    "    dir_str = 'histeq'\n",
    "    \n",
    "model_dir = '../saved_models/' + pre + dir_str + '/' + train_set + '/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "if norm == 'zca_original':\n",
    "    zca_transform = zca.compute_zca(train_set)\n",
    "else:\n",
    "    zca_transform = None\n",
    "    \n",
    "ds_train, ds_val, _ = dataset_utils.get_dataset(\n",
    "      train_set,\n",
    "      32,\n",
    "      mode,\n",
    "      normalize=norm,\n",
    "      dequantize=False,\n",
    "      visible_dist='categorical',\n",
    "      zca_transform=zca_transform,\n",
    "      mutation_rate=mutation_rate\n",
    "  )\n",
    "\n",
    "dist = network.PixelCNN(\n",
    "      image_shape=input_shape,\n",
    "      num_resnet=num_resnet,\n",
    "      num_hierarchies=num_hierarchies,\n",
    "      num_filters=num_filters,\n",
    "      num_logistic_mix=num_logistic_mix,\n",
    "      dropout_p=dropout_p,\n",
    "      use_weight_norm=use_weight_norm,\n",
    "      reg_weight=reg_weight\n",
    ")\n",
    "\n",
    "image_input = tfkl.Input(shape=input_shape)\n",
    "log_prob = dist.log_prob(image_input)\n",
    "model = tfk.Model(inputs=image_input, outputs=log_prob)\n",
    "model.add_loss(-tf.reduce_mean(log_prob))\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "model.build([None] + list(input_shape))\n",
    "# model.load_weights(model_dir+'weights')\n",
    "\n",
    "# model.fit(\n",
    "#         ds_train,\n",
    "#         epochs=epochs,\n",
    "#         validation_data=ds_val,\n",
    "#         callbacks=[\n",
    "#             utils.CNSModelCheckpoint(\n",
    "#                 filepath=os.path.join(model_dir+'weights'),\n",
    "#                 verbose=1, save_weights_only=True, save_best_only=True\n",
    "#                 )\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "model.load_weights(model_dir+'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048036a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/102 [00:00<?, ?it/s]2022-05-09 18:10:26.582641: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-05-09 18:10:28.067800: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function WeightNorm.call at 0x7fd1dc7abee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function WeightNorm.call at 0x7fd1dc7abee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function WeightNorm.call at 0x7fd1e410ef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function WeightNorm.call at 0x7fd1e410ef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [02:49<00:00,  1.66s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:04<00:00,  1.62s/it]\n",
      "156it [04:17,  1.65s/it]\n",
      "50it [01:21,  1.63s/it]\n",
      "53it [04:42,  5.33s/it]\n",
      "40it [00:32,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "\n",
    "if train_set == 'emnist/letters':\n",
    "    train_set = 'emnist_letters'\n",
    "    \n",
    "probs = {}\n",
    "if bg:\n",
    "    if which_arch == 'lrat_arch':\n",
    "        probs1 = loadmat('../probs/lrat_arch/' + dir_str + '/' + train_set + '.mat')\n",
    "    else:\n",
    "        probs1 = loadmat('../probs/lrat/' + dir_str + '/' + train_set + '.mat')\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    _, _, ds_test = dataset_utils.get_dataset(\n",
    "          dataset,\n",
    "          256,\n",
    "          mode,\n",
    "          normalize=norm,\n",
    "          dequantize=False,\n",
    "          visible_dist='categorical',\n",
    "          zca_transform=zca_transform,\n",
    "          mutation_rate=0\n",
    "      )\n",
    "    tmp = []\n",
    "    for test_batch in tqdm.tqdm(ds_test):\n",
    "        tmp.append(dist.log_prob(tf.cast(test_batch, tf.float32),\n",
    "                                        training=False).numpy())\n",
    "\n",
    "    tmp = np.expand_dims(np.concatenate(tmp, axis=0),axis=-1)\n",
    "    tmp = np.array(tmp)\n",
    "    \n",
    "    if bg:\n",
    "        probs[dataset] = probs1[dataset] - tmp\n",
    "    else:\n",
    "        probs[dataset] = tmp\n",
    "\n",
    "save_dir = '../probs/' + pre + dir_str + '/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "savemat(save_dir + train_set + '.mat', probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
